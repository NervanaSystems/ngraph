/*******************************************************************************
* Copyright 2017-2018 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

#include "ngraph/pass/liveness.hpp"

#include "ngraph/descriptor/input.hpp"
#include "ngraph/descriptor/output.hpp"
#include "ngraph/function.hpp"
#include "ngraph/node.hpp"
#include "ngraph/op/constant.hpp"
#include "ngraph/op/parameter.hpp"
#include "ngraph/op/result.hpp"

using namespace std;
using namespace ngraph;

static void get_op_input_tensors(ngraph::Node& n, vector<descriptor::Tensor*>& tensors)
{
    tensors.clear();

    auto& inputs = n.get_inputs();
    tensors.reserve(inputs.size());

    for (descriptor::Input& input_decl : inputs)
    {
        descriptor::Tensor* t = &(input_decl.get_tensor());
        tensors.push_back(t);
    }
}

static void get_op_output_tensors(ngraph::Node& n, vector<descriptor::Tensor*>& tensors)
{
    tensors.clear();
    tensors.reserve(n.get_output_size());

    for (size_t i = 0; i < n.get_output_size(); ++i)
    {
        descriptor::Tensor* t = &(n.get_output_tensor(i));
        tensors.push_back(t);
    }
}

static vector<Node*> get_ordered_ops_vector(ngraph::Function& f)
{
    vector<Node*> ops_schedule;

    const list<shared_ptr<Node>> ops = f.get_ordered_ops();
    ops_schedule.reserve(ops.size());
    for (const shared_ptr<Node>& n : ops)
    {
        ops_schedule.push_back(n.get());
    }

    return ops_schedule;
}

static unordered_set<descriptor::Tensor*> get_persistent_tensors(ngraph::Function& f,
                                                                 const vector<Node*>& ops)
{
    unordered_set<descriptor::Tensor*> tensors;

    // Tensors generated by ngraph Constant ops...
    for (const Node* n : ops)
    {
        if (auto constant_node = dynamic_cast<const op::Constant*>(n))
        {
            for (size_t i = 0; i < constant_node->get_output_size(); ++i)
            {
                descriptor::Tensor& t = constant_node->get_output_tensor(i);
                tensors.insert(&t);
            }
        }
    }

    // Tensors to provide the parameters of the graph function...
    for (const shared_ptr<op::Parameter>& n : f.get_parameters())
    {
        for (size_t i = 0; i < n->get_output_size(); ++i)
        {
            descriptor::Tensor& t = n->get_output_tensor(i);
            tensors.insert(&t);
        }
    }

    // Tensors to receive the results of the graph function...
    for (const shared_ptr<op::Result>& n : f.get_results())
    {
        for (size_t i = 0; i < n->get_output_size(); ++i)
        {
            descriptor::Tensor& t = n->get_output_tensor(i);
            tensors.insert(&t);
        }
    }

    return tensors;
}

bool pass::Liveness::run_on_function(shared_ptr<ngraph::Function> function)
{
    //------------------------------------------------------------------------------------------------
    // ALGORITHM OVERVIEW:
    // This pass uses a backwards-dataflow analysis, very similar to what one would find in textbook
    // descriptions of liveness analysis.
    // Note that we visit the ops in *reverse* execution order.  So we visit the final use (if there
    // is one) of each non-persistent tensor before visiting the op that generates that tensor.
    //------------------------------------------------------------------------------------------------

    ngraph::Function& f = *(function.get());

    // FIXME: Ideally this should be computed just once and passed in to the function...
    const vector<Node*> ops_execution_order = get_ordered_ops_vector(f);

    const unordered_set<descriptor::Tensor*> persistent_tensors =
        get_persistent_tensors(f, ops_execution_order);

    unordered_set<descriptor::Tensor*> live_nonpersistent_tensors;

    // These vectors are defined outside the 'for' loop only to avoid repeated memory alloc/dealloc...
    vector<descriptor::Tensor*> op_input_tensors;
    vector<descriptor::Tensor*> op_output_tensors;

    for (auto op_iter = ops_execution_order.rbegin(); op_iter != ops_execution_order.rend();
         ++op_iter)
    {
        Node& op = **op_iter;
        op.liveness_new_list.clear();
        op.liveness_free_list.clear();

        get_op_input_tensors(op, op_input_tensors);
        get_op_output_tensors(op, op_output_tensors);

        //----------------------------------------------------------------------------------------------
        // Deal with any tensors for which 'op' is the final user...
        //----------------------------------------------------------------------------------------------
        for (descriptor::Tensor* t : op_input_tensors)
        {
            if (persistent_tensors.find(t) != persistent_tensors.end())
            {
                continue;
            }

            const auto insert_outcome = live_nonpersistent_tensors.insert(t);
            if (insert_outcome.second)
            {
                // The insert operation succeeded, which means the set did NOT already have an entry for
                // 't'.  It logically follows, then, that 'op' is be the final node to use 't'.
                op.liveness_free_list.insert(t);
            }
        }

        //----------------------------------------------------------------------------------------------
        // Deal with any tensors for which 'op' is the creator...
        //----------------------------------------------------------------------------------------------
        for (descriptor::Tensor* t : op_output_tensors)
        {
            if (persistent_tensors.find(t) != persistent_tensors.end())
            {
                continue;
            }

            op.liveness_new_list.insert(t);

            const bool t_has_users = (live_nonpersistent_tensors.erase(t));
            if (!t_has_users)
            {
                // Our convention is that a (non-persistent) tensor with no uses get reported in both
                // the 'liveness_new_list' *and* 'liveness_free_list' of the op that produces it.
                op.liveness_free_list.insert(t);
            }
        }
    }

    return false;
}
