# Tests not supported by the PlaidML backend
backwards_reverse_sequence_n3_c2_h3     # No plans to implement ReverseSequence
backwards_reverse_sequence_n4d2c3h2w2   # No plans to implement ReverseSequence
max_matrix_rows_zero                    # Out-of-range for PlaidML
max_matrix_rows_zero_int32              # Out-of-range for PlaidML
max_matrix_cols_zero                    # Out-of-range for PlaidML
max_vector_zero                         # Out-of-range for PlaidML
max_matrix_to_scalar_zero_by_zero       # Out-of-range for PlaidML
max_3d_eliminate_zero_dim               # Out-of-range for PlaidML
min_matrix_rows_zero                    # Out-of-range for PlaidML
min_matrix_cols_zero                    # Out-of-range for PlaidML
min_vector_zero                         # Out-of-range for PlaidML
min_matrix_to_scalar_zero_by_zero       # Out-of-range for PlaidML
min_3d_eliminate_zero_dim               # Out-of-range for PlaidML
reverse_sequence_n2c3h4w2               # No plans to implement ReverseSequence
reverse_sequence_n4c3h2w2               # No plans to implement ReverseSequence
reverse_sequence_n4d2c3h2w2             # No plans to implement ReverseSequence
topk_1d_max_all                         # No plans to implement TopK
topk_1d_max_partial                     # No plans to implement TopK
topk_1d_max_one                         # No plans to implement TopK
topk_1d_min_all                         # No plans to implement TopK
topk_1d_min_partial                     # No plans to implement TopK
topk_1d_min_one                         # No plans to implement TopK
topk_3d_large_input_max                 # No plans to implement TopK
topk_3d_large_input_min                 # No plans to implement TopK
topk_3d_max_all                         # No plans to implement TopK
topk_3d_max_partial                     # No plans to implement TopK
topk_3d_max_one                         # No plans to implement TopK
topk_3d_min_all                         # No plans to implement TopK
topk_3d_min_partial                     # No plans to implement TopK
topk_3d_min_one                         # No plans to implement TopK
topk_3d_single_output                   # No plans to implement TopK
topk_2d_max_all                         # No plans to implement TopK
topk_2d_max_partial                     # No plans to implement TopK
topk_2d_max_one                         # No plans to implement TopK
topk_2d_min_all                         # No plans to implement TopK
topk_2d_min_partial                     # No plans to implement TopK
topk_2d_min_one                         # No plans to implement TopK
topk_int64                              # No plans to implement TopK
topk_5d_max_partial                     # No plans to implement TopK

# Tests that PlaidML might be able to run at some point.
backwards_maxpool_n2_c1_hw5_3x3_str2_max_pad1x2_2x3
backwards_maxpool_n4c1h4w4_kh2kw2_sh1sw1
backwards_maxpool_n2c1h5w5_kh3kw3_sh2sw2
backwards_maxpool_n4_c1_hw4_2x2_max
backwards_maxpool_n2_c1_hw5_3x3_str2_max
backwards_slice
batchnorm_fprop_bprop  # To debug
batchnorm_fprop_bprop_2step  # To debug
batch_norm_inference_0eps_f64
batch_norm_inference_f64
batch_norm_training_0eps_f64
softmax_axis_3d_double  # To debug
replace_slice_matrix_inplace
max_pool_2d_1channel_1image_overpadded
max_pool_3d
maxpool_bprop_larger_than_cache
generate_mask
generate_mask2
avg_pool_3d
avg_pool_3d_uneven_strided_padded_include_in_computation
quantize_clamp_int32                    # Requires fp64 inputs, which won't work on GPUs
numeric_float_nan
numeric_double_nan
shape_of_scalar
shape_of_vector
shape_of_matrix
shape_of_5d
sum_stable_acc_double  # To debug: precision errors
embedding_lookup_4x5_reverse
embedding_lookup_10x1_arbitrary
embedding_lookup_10x1_arbitrary_index_type_int
embedding_lookup_10x1_arbitrary_index_type_int64
floor_int32
gather_nd_scalar_from_2d
gather_nd_1d_from_2d
gather_nd_scalar_from_3d
gather_nd_1d_from_3d
gather_nd_2d_from_3d
gather_nd_batch_scalar_from_2d
gather_nd_batch_1d_from_2d
gather_nd_batch_scalar_from_3d
gather_nd_batch_1d_from_3d
gather_nd_batch_2d_from_3d
gather_nd_single_indices
gather_4d_indices_no_axis_uint8
gather_scalar_indices_axis_1_2d_input
gather_1d_indices_axis_2_4d_input
gather_2d_indices_axis_1_2d_input
gather_scalar_indices_no_axis_2d_input
gather_1d_indices_no_axis_1d_input
gather_2d_indices_no_axis_2d_input
gather_3d_indices_no_axis_2d_input
gather_4d_indices_no_axis_2d_input
scatter_add_4d_indices
scatter_add_3d_indices
scatter_add_2d_indices
scatter_add_1d_indices
scatter_add_scalar_indices
scatter_nd_add_batch_2d_to_3d
scatter_nd_add_2d_to_3d

# To be triaged -- bad kernels, numerical accuracy, edge conditions,
# unimplemented functionality, &c
cos
erf
sin
tan
not
abc_int64
concat_matrix_int64
select_double
convert_int32_bool
convert_float32_bool
tensor_constant_int64
constant_equality_bool
numeric_float_inf
numeric_double_inf
computation_reuse
pad_negative_exterior_1d_check_limits
pad_edge_1d
pad_edge_1d_top_neg
pad_edge_1d_top_neg_bigger_than_tensor
pad_edge_1d_bottom_neg
pad_edge_1d_bottom_neg_bigger_than_tensor
pad_edge_2d
pad_edge_2d_with_neg
pad_reflect_1d
pad_reflect_1d_top_neg
pad_reflect_1d_top_neg_bigger_than_tensor
pad_reflect_1d_bottom_neg
pad_reflect_1d_bottom_neg_bigger_than_tensor
pad_reflect_1d_multi_reflect
pad_reflect_2d
pad_reflect_2d_with_neg
pad_negative_exterior_2d
pad_negative_exterior_2d_all_negative
pad_negative_exterior_4d
max_trivial_int8
max_trivial_5d_int32
max_3d_to_scalar_double
softmax_axis_3d
logical_and
logical_or
batch_norm_inference_parameters_duplication
batch_norm_fprop_b1c2h2w2
batch_norm_fprop_b2c2h2w1
batchnorm_fprop_b2c2d2h1w1
batch_norm_fprop_inference_b2c2h2w1
argmax_3D_axis_0
argmax_3D_axis_1
argmax_3D_axis_2
argmin_trivial_in_double
topk_2d_max_one_with_equal_values
sum_trivial_in_double
sum_stable_simple_double
one_hot_vector_many_categories
gather_no_axis_int8
gather_no_axis_int16
gather_no_axis_int32
gather_no_axis_int64
gather_no_axis_uint8
gather_no_axis_uint16
gather_no_axis_uint32
gather_no_axis_uint64
gather_no_axis_bool
elu
elu_negative_alpha
prelu
hardsigmoid
prelu_shared_slope
prelu_negative_slope
relu_2Dfprop_i32
conv_bias_1d
conv_bias_2d
conv_bias_3d
conv_bias_bprop_2d
conv_bias_add_2d
group_conv
space_to_depth
depth_to_space
normalize_across_chw_scalar_scale_4d
normalize_across_chw_scalar_scale_3d
normalize_across_chw_scalar_scale_2d
normalize_across_chw_w_scale
gemm
fused_clamp
mvn_mean_normalization
mvn_mean_normalization_split_channels
mvn_mean_variance_normalization
mvn_mean_variance_normalization_split_channels
grn_4d
grn_2d_with_bias
scale_shift_no_broadcast
scale_shift
shuffle_channels_simple
shuffle_channels_negative_axis
shuffle_channels_float
squeeze
squeeze_default_axes
squared_difference
squared_difference_broadcast
fake_quantize
fake_quantize_with_clip
fake_quantize_with_clip_across_channels
dot_0_0
dot_2x0_0
equal
notequal
greater
greater_int64
greatereq
less
lesseq
lesseq_int32
lesseq_bool
broadcast_vector_rowwise_int64
minimum_int64
maximum_int64
auto_bcast_binary_elementwise
any_trivial
any_2x2x3_eliminate_dim_0
backwards_acos
backwards_asin
backwards_atan
backwards_softmax_all
backwards_softmax_axis
backwards_softmax_underflow
backwards_softmax_3d
batch_mat_mul_forward
dot_matrix_2x0_0x2
gelu_f32
gelu_f64
# From onnx tests
model_quant_conv_linear_2d
model_quant_conv_linear_3d
model_conv_integer
model_conv_integer_zero_point_zero
model_conv_integer_pads
model_lstm_fwd_large_batch_no_clip
model_global_lp_pool_p3
model_argmin_no_keepdims
model_elu
model_selu
model_sigmoid
model_softplus
model_argmax_int32
model_argmin_int32
model_lp_norm_default
model_instance_normalization

# failings on plaidml_nGPU
argmin_4D_i64
argmax_4D_i64

# dgkutnic ww24.5: these tests are to be triaged by the PlaidML team
# ww25.2: re-scrubbed this list of tests after fixing check_inputs
# initial debug points to some of these failing due to precision issues
sqrt
batch_norm_inference_0eps_f32
batch_norm_inference_f32
batch_norm_training_0eps_f32
argmin_trivial
argmax_trivial
argmin_trivial_in_i32
argmin_3D_i32
argmin_3D_i64
argmax_3D_i32
argmax_3D_i64
sum_large_1d_to_scalar
sum_stable_acc
one_hot_scalar_2_in_3
one_hot_scalar_1_in_3
one_hot_scalar_0_in_3
lstm_cell_no_bias_no_peepholes
lstm_cell_bias_peepholes
lstm_cell_bias_peepholes_clip_input_forget
lstm_cell_activaction_functions
group_conv_transpose
group_conv_transpose_output_shape
divide_python_rounding_int32
backwards_batchmatmul_tensor2_tensor2

# unsupported ops: 'QuantizedConvolution', 'QuantizedDot', 'TopK', 'Erf', 'EmbeddingLookup'
model_quant_conv_linear
model_conv_integer_no_zero_point
model_matmul_integer_no_zero_point
model_matmul_integer_4d_no_zero_point
model_top_k
model_erf
model_erf_int32
model_hardmax
quantized_convolution
quantized_conv_int32_output

# unsupported op: `ReverseSequence`
model_lstm_bdir_short_input_seq
model_lstm_mixed_seq_reverse
model_reverse_sequence_0_batch_1
model_reverse_sequence_1_batch_0

# node validation error: "Argument shapes are inconsistent."
model_lstm_fwd_with_clip
model_lstm_fwd_mixed_seq
model_lstm_fwd_hardsigmoid_activation
model_reduce_log_sum
model_reduce_log_sum_exp
model_reduce_mean

# result mismatch
model_dequantize_linear_scalar_zero_scale_int8
model_softmax
avg_pool_3d_uneven_strided_padded
rnn_cell_activation_function
gru_cell_bias_clip
gru_cell_linear_before_reset

# After https://github.com/NervanaSystems/ngraph/pull/3262, these tests began
# failing with what appear to be precision issues. That PR simply split the
# old "backend_test.in.cpp" into multiple files. The only relevant side effect
# I can think of here is that the order of test execution changed as a result.
softmax_all
softmax_axis
softmax_underflow
softmax_overflow
sigmoid_n1c1h2w2
sigmoid_n1c1h4
sigmoid_bprop_n1c1h4
lrn
