# Tests not supported by the PlaidML backend
backwards_reverse_sequence_n3_c2_h3     # No plans to implement ReverseSequence
backwards_reverse_sequence_n4d2c3h2w2   # No plans to implement ReverseSequence
max_matrix_rows_zero                    # Out-of-range for PlaidML
max_matrix_rows_zero_int32              # Out-of-range for PlaidML
max_matrix_cols_zero                    # Out-of-range for PlaidML
max_vector_zero                         # Out-of-range for PlaidML
max_matrix_to_scalar_zero_by_zero       # Out-of-range for PlaidML
max_3d_eliminate_zero_dim               # Out-of-range for PlaidML
min_matrix_rows_zero                    # Out-of-range for PlaidML
min_matrix_cols_zero                    # Out-of-range for PlaidML
min_vector_zero                         # Out-of-range for PlaidML
min_matrix_to_scalar_zero_by_zero       # Out-of-range for PlaidML
min_3d_eliminate_zero_dim               # Out-of-range for PlaidML
reverse_sequence_n2c3h4w2               # No plans to implement ReverseSequence
reverse_sequence_n4c3h2w2               # No plans to implement ReverseSequence
reverse_sequence_n4d2c3h2w2             # No plans to implement ReverseSequence
topk_1d_max_all                         # No plans to implement TopK
topk_1d_max_partial                     # No plans to implement TopK
topk_1d_max_one                         # No plans to implement TopK
topk_1d_min_all                         # No plans to implement TopK
topk_1d_min_partial                     # No plans to implement TopK
topk_1d_min_one                         # No plans to implement TopK
topk_3d_large_input_max                 # No plans to implement TopK
topk_3d_large_input_min                 # No plans to implement TopK
topk_3d_max_all                         # No plans to implement TopK
topk_3d_max_partial                     # No plans to implement TopK
topk_3d_max_one                         # No plans to implement TopK
topk_3d_min_all                         # No plans to implement TopK
topk_3d_min_partial                     # No plans to implement TopK
topk_3d_min_one                         # No plans to implement TopK
topk_3d_single_output                   # No plans to implement TopK
topk_2d_max_all                         # No plans to implement TopK
topk_2d_max_partial                     # No plans to implement TopK
topk_2d_max_one                         # No plans to implement TopK
topk_2d_min_all                         # No plans to implement TopK
topk_2d_min_partial                     # No plans to implement TopK
topk_2d_min_one                         # No plans to implement TopK
topk_int64                              # No plans to implement TopK
topk_5d_max_partial                     # No plans to implement TopK
topk_1d_i32_max_all                     # No plans to implement TopK
topk_resnet50                           # No plans to implement TopK
topk_max_sort_none                      # No plans to implement TopK
topk_min_sort_none                      # No plans to implement TopK
topk_max_sort_value                     # No plans to implement TopK
topk_min_sort_value                     # No plans to implement TopK
topk_max_sort_index                     # No plans to implement TopK
topk_min_sort_index                     # No plans to implement TopK
topk_2d_max_one_with_equal_values       # No plans to implement TopK
model_top_k                             # No plans to implement TopK
top_k_opset_10                          # No plans to implement TopK
top_k_opset_10_const_k                  # No plans to implement TopK
top_k_opset_11_const_k_smallest         # No plans to implement TopK

# unsupported op: `Erf`
erf
gelu_f32
gelu_f64
gelu_backprop_factor_f32
gelu_backprop_factor_f64
backwards_gelu_f32
backwards_gelu_f64
model_erf
model_erf_int32

# unsupported ops: `BroadcastDistributed`
broadcastdistributed

# unsupported ops: 'QuantizedConvolution', 'QuantizedDot', 'EmbeddingLookup'
model_quant_conv_linear
model_conv_integer_no_zero_point
model_matmul_integer_no_zero_point
model_matmul_integer_4d_no_zero_point
model_qlinear_matmul
model_qlinear_matmul_3d
model_matmul_integer
model_matmul_integer_zero_point_zero
model_matmul_integer_scalar
model_matmul_integer_4d
model_matmul_integer_4d_zero_point
model_hardmax
quantized_convolution
quantized_conv_int32_output
quantized_dot_u8u8
quantized_dot_int32_output
embedding_lookup_4x5_reverse
embedding_lookup_10x1_arbitrary
embedding_lookup_10x1_arbitrary_index_type_int
embedding_lookup_10x1_arbitrary_index_type_int64

# unsupported op: `ReverseSequence`
model_lstm_bdir_short_input_seq
model_lstm_mixed_seq_reverse
model_reverse_sequence_0_batch_1
model_reverse_sequence_1_batch_0

# unsupported broadcast mode (pdpd)
fake_quantize_pdpd
auto_bcast_binary_elementwise_pdpd
auto_bcast_binary_elementwise_pdpd_dynamic

# result mismatch
model_dequantize_linear_scalar_zero_scale_int8
model_softmax
avg_pool_3d_uneven_strided_padded
rnn_cell_activation_function
gru_cell_bias_clip
gru_cell_linear_before_reset
softmax_axis_3d
relu_2Dfprop_i32
avg_pool_uint8
avg_pool_int8
one_hot_vector_many_categories
conv_bias_1d
conv_bias_2d
conv_bias_3d
conv_bias_add_2d
normalize_across_hw_4d
divide_python_rounding_int32
convert_int32_bool
convert_float32_bool
batch_norm_inference_0eps_f64
batch_norm_inference_f64
batch_norm_training_0eps_f64
batch_norm_inference_parameters_duplication
batch_norm_fprop_b1c2h2w2
batch_norm_fprop_b2c2h2w1
batch_norm_fprop_b2c2d2h1w1
batch_norm_fprop_inference_b2c2h2w1
pad_edge_1d
pad_edge_1d_top_neg
pad_edge_1d_top_neg_bigger_than_tensor
pad_edge_1d_bottom_neg
pad_edge_1d_bottom_neg_bigger_than_tensor
pad_edge_2d
pad_edge_2d_with_neg
pad_reflect_1d
pad_reflect_1d_top_neg
pad_reflect_1d_top_neg_bigger_than_tensor
pad_reflect_1d_bottom_neg
pad_reflect_1d_bottom_neg_bigger_than_tensor
pad_reflect_1d_multi_reflect
pad_reflect_2d
pad_reflect_2d_with_neg
pad_symmetric

# No double precision FP support in PlaidML
sum_trivial_in_double
sum_stable_acc_double
sum_stable_simple_double
softmax_axis_3d_double
select_double
numeric_double_nan
numeric_double_inf
max_3d_to_scalar_double
argmin_trivial_in_double

# unsupported op: `ShapeOf`
shape_of_vector
shape_of_matrix
shape_of_5d

# unsupported ops: `ScatterAdd` `ScatterNDAdd`
scatter_add_3d_indices
scatter_add_2d_indices
scatter_add_1d_indices
scatter_add_scalar_indices
scatter_nd_add_batch_2d_to_3d
scatter_nd_add_2d_to_3d

# c++ runtime exception
replace_slice_matrix_inplace
quantize_clamp_int32
pad_negative_exterior_1d_check_limits
backwards_slice
backwards_softmax_3d
argmin_3D_i32
argmin_3D_i64
argmax_3D_i32
argmax_3D_i64
argmax_3D_axis_0
argmax_3D_axis_1
argmax_3D_axis_2
argmin_4D_i64
argmax_4D_i64

# PlaidML doesn't support over-padded MaxPool
max_pool_3d
backwards_maxpool_n2_c1_hw5_3x3_str2_max_pad1x2_2x3

# c++ compilation failure
max_trivial_int8
max_trivial_5d_int32
floor_int32
any_trivial
any_2x2x3_eliminate_dim_0
backwards_batch_norm_training_3d

# unsupported op: `GenerateMask`
generate_mask
generate_mask2

# unsupported op: `Gather`
gather_4d_indices_no_axis_uint8
gather_4d_indices_no_axis_2d_input
gather_3d_indices_no_axis_2d_input
gather_2d_indices_no_axis_2d_input
gather_2d_negative_and_positive_indices_no_axis_2d_input
gather_1d_indices_no_axis_1d_input
gather_scalar_indices_no_axis_2d_input
gather_2d_indices_axis_1_2d_input
gather_1d_indices_axis_2_4d_input
gather_scalar_indices_axis_1_2d_input
gather_nd_single_indices
gather_nd_scalar_from_2d
gather_nd_1d_from_2d
gather_nd_scalar_from_3d
gather_nd_1d_from_3d
gather_nd_2d_from_3d
gather_nd_batch_scalar_from_2d
gather_nd_batch_1d_from_2d
gather_nd_batch_scalar_from_3d
gather_nd_batch_1d_from_3d
gather_nd_batch_2d_from_3d
gather_no_axis_int8
gather_no_axis_int16
gather_no_axis_int32
gather_no_axis_int64
gather_no_axis_uint8
gather_no_axis_uint16
gather_no_axis_uint32
gather_no_axis_uint64
gather_no_axis_bool

# unsupported op: `BatchMatMul`
batch_mat_mul_forward
backwards_batchmatmul_tensor2_tensor2

# onnx tests
model_quant_conv_linear_2d
model_quant_conv_linear_3d
model_conv_integer
model_conv_integer_zero_point_zero
model_conv_integer_pads
model_lstm_fwd_hardsigmoid_activation
model_lstm_fwd_with_clip
model_lstm_fwd_mixed_seq
model_lstm_fwd_large_batch_no_clip
model_global_lp_pool_p3
model_argmin_no_keepdims
model_reduce_log_sum_exp
model_elu
model_selu
model_sigmoid
model_softplus
model_argmax_int32
model_argmin_int32
model_lp_norm_default
model_instance_normalization

# passing locally, fails closeness checks in CI which may be too strict
elu
elu_negative_alpha
lstm_cell_no_bias_no_peepholes
lstm_cell_bias_peepholes
lstm_cell_bias_peepholes_clip_input_forget
lstm_cell_activaction_functions
dot_0_0
dot_matrix_2x0_0x2
dot_2x0_0
auto_bcast_binary_elementwise
max_pool_2d_1channel_1image_overpadded

# node validation error: "Applying function, tensor with mismatching dimensionality: F, expected=4, got=5"
group_conv_groups_included_in_shape

# passes locally, fails in CI
numeric_float_nan
fake_quantize_with_clip_across_channels

# axes input param not supported
lrn_across_h
lrn_across_hw
lrn_across_all_dims
lrn_across_nw
lrn_across_empty
lrn_6D_across_2_axes

# RandomUniform not supported in PlaidML backend
random_uniform_all_static_seed_unused
random_uniform_all_static_seed_used
random_uniform_seed_use_dynamic
random_uniform_all_static_range_dynamic
random_uniform_dynamic_shapes

# Fused op test fails on mac
layer_norm_affine_stats
layer_norm_bprop_affine_stats
layer_norm_bprop_affine

# shapes with zeros dimensions like (5, 0, 5) not supported in PlaidML backend
dyn_replace_slice

# bf16 test cases not supported
convert_float32_bf16
convert_bf16_float32

# infinitive values are returned for below cases
normalize_across_c_2x2_shape
normalize_across_c_2x4_shape

# dyn shape
dyn_generate_mask
